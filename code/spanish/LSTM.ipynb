{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acfcc9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee36fb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Embedding, GRU, SimpleRNN, RNN, LSTM, Flatten, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, precision_recall_fscore_support, accuracy_score\n",
    "# from baseline import bong\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a89034da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_file, test_file, padding='post'):\n",
    "    list_of_sentences_train = list()\n",
    "    list_of_hateful_train = list()\n",
    "    list_of_aggressive_train = list()\n",
    "    list_of_targeted_train = list()\n",
    "    max_length = list()\n",
    "\n",
    "    ### Train file ###\n",
    "    with open(train_file, encoding=\"utf8\") as cur_train_file:\n",
    "        reader = csv.reader(cur_train_file, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            list_of_sentences_train.append(row[1])\n",
    "            list_of_hateful_train.append(row[2])\n",
    "            list_of_targeted_train.append(row[3])\n",
    "            list_of_aggressive_train.append(row[4])\n",
    "\n",
    "    list_of_sentences_train.pop(0)\n",
    "    list_of_hateful_train.pop(0)\n",
    "    list_of_targeted_train.pop(0)\n",
    "    list_of_aggressive_train.pop(0)\n",
    "\n",
    "    for x in list_of_sentences_train:\n",
    "        max_length.append(len(x))\n",
    "    longest_sent = max(max_length)\n",
    "\n",
    "    alphabet = dict()\n",
    "    train_words = []\n",
    "\n",
    "    for sent in list_of_sentences_train:\n",
    "        words = []\n",
    "        for char in sent:\n",
    "            if char in alphabet:\n",
    "                words.append(alphabet[char])\n",
    "            else:\n",
    "                alphabet[char] = len(alphabet) + 1\n",
    "                words.append(alphabet[char])\n",
    "\n",
    "        train_words.append(words)\n",
    "\n",
    "    train_x = train_words\n",
    "    #train_y_hateful = list_of_hateful_train\n",
    "    train_y_hateful = [int(label) for label in list_of_hateful_train]\n",
    "    train_y_targeted = list_of_targeted_train\n",
    "    train_y_aggressive = list_of_aggressive_train\n",
    "\n",
    "    train_x = pad_sequences(train_x, padding=padding, value=0, maxlen=longest_sent, truncating='post')\n",
    "\n",
    "    ### TEST FIlE ###\n",
    "\n",
    "    list_of_sentences_test = list()\n",
    "    list_of_hateful_test = list()\n",
    "    list_of_aggressive_test = list()\n",
    "    list_of_targeted_test = list()\n",
    "\n",
    "    with open(test_file, encoding=\"utf8\") as cur_test_file:\n",
    "        reader = csv.reader(cur_test_file, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            list_of_sentences_test.append(row[1])\n",
    "            list_of_hateful_test.append(row[2])\n",
    "            list_of_aggressive_test.append(row[3])\n",
    "            list_of_targeted_test.append(row[4])\n",
    "\n",
    "        list_of_sentences_test.pop(0)\n",
    "        list_of_hateful_test.pop(0)\n",
    "        list_of_aggressive_test.pop(0)\n",
    "        list_of_targeted_test.pop(0)\n",
    "\n",
    "        test_words = []\n",
    "        alphabet_length = len(alphabet)\n",
    "\n",
    "        for test_sent in list_of_sentences_test:\n",
    "            words = []\n",
    "            for char_test in test_sent:\n",
    "                if char_test in alphabet:\n",
    "                    words.append(alphabet[char_test])\n",
    "                else:\n",
    "                    unk = alphabet_length + 1\n",
    "                    words.append(unk)\n",
    "\n",
    "            test_words.append(words)\n",
    "\n",
    "        test_x = test_words\n",
    "        #test_y_hateful = list_of_hateful_test\n",
    "        test_y_hateful = [int(label) for label in list_of_hateful_test]\n",
    "        test_y_aggressive = list_of_aggressive_test\n",
    "        test_y_targeted = list_of_targeted_test\n",
    "\n",
    "        test_x = pad_sequences(test_x, padding=padding, value=0, maxlen=longest_sent, truncating='post')\n",
    "\n",
    "        print(len(train_x), len(train_y_hateful),len(train_y_aggressive), len(train_y_targeted), len(test_x),\n",
    "             len(test_y_hateful), len(test_y_aggressive), len(test_y_targeted))\n",
    "\n",
    "        return train_x, train_y_hateful, train_y_aggressive, train_y_targeted, test_x, test_y_hateful, test_y_aggressive, test_y_targeted, longest_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04ad1a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92f35669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4450 4450 4450 4450 500 500 500 500\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "Epoch 1/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.7198 - accuracy: 0.5679WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "35/35 [==============================] - 283s 8s/step - loss: 0.7198 - accuracy: 0.5679\n",
      "Epoch 2/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.6662 - accuracy: 0.6043WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "35/35 [==============================] - 296s 9s/step - loss: 0.6662 - accuracy: 0.6043\n",
      "Epoch 3/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.6569 - accuracy: 0.6142WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "35/35 [==============================] - 285s 8s/step - loss: 0.6569 - accuracy: 0.6142\n",
      "Epoch 4/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.6324 - accuracy: 0.6373 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "35/35 [==============================] - 354s 10s/step - loss: 0.6324 - accuracy: 0.6373\n",
      "Epoch 5/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.6105 - accuracy: 0.6753 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "35/35 [==============================] - 343s 10s/step - loss: 0.6105 - accuracy: 0.6753\n",
      "Epoch 6/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5943 - accuracy: 0.6899WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "35/35 [==============================] - 249s 7s/step - loss: 0.5943 - accuracy: 0.6899\n",
      "Epoch 7/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5862 - accuracy: 0.6962WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "35/35 [==============================] - 249s 7s/step - loss: 0.5862 - accuracy: 0.6962\n",
      "Epoch 8/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5788 - accuracy: 0.7004WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "35/35 [==============================] - 249s 7s/step - loss: 0.5788 - accuracy: 0.7004\n",
      "Epoch 9/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5716 - accuracy: 0.7110WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "35/35 [==============================] - 279s 8s/step - loss: 0.5716 - accuracy: 0.7110\n",
      "Epoch 10/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5627 - accuracy: 0.7175WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "35/35 [==============================] - 231s 7s/step - loss: 0.5627 - accuracy: 0.7175\n",
      "Epoch 11/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5606 - accuracy: 0.7160 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "35/35 [==============================] - 357s 10s/step - loss: 0.5606 - accuracy: 0.7160\n",
      "Epoch 12/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5509 - accuracy: 0.7216WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "35/35 [==============================] - 294s 8s/step - loss: 0.5509 - accuracy: 0.7216\n",
      "Epoch 13/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5369 - accuracy: 0.7373WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "35/35 [==============================] - 341s 9s/step - loss: 0.5369 - accuracy: 0.7373\n",
      "Epoch 14/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5427 - accuracy: 0.7229 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "35/35 [==============================] - 452s 13s/step - loss: 0.5427 - accuracy: 0.7229\n",
      "Epoch 15/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5260 - accuracy: 0.7425WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "35/35 [==============================] - 308s 9s/step - loss: 0.5260 - accuracy: 0.7425\n",
      "Epoch 16/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5127 - accuracy: 0.7499 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "35/35 [==============================] - 355s 10s/step - loss: 0.5127 - accuracy: 0.7499\n",
      "Epoch 17/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5053 - accuracy: 0.7557 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "35/35 [==============================] - 431s 13s/step - loss: 0.5053 - accuracy: 0.7557\n",
      "Epoch 18/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.5053 - accuracy: 0.7533 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "35/35 [==============================] - 404s 12s/step - loss: 0.5053 - accuracy: 0.7533\n",
      "Epoch 19/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4872 - accuracy: 0.7733 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "35/35 [==============================] - 389s 11s/step - loss: 0.4872 - accuracy: 0.7733\n",
      "Epoch 20/20\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4730 - accuracy: 0.7739WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "35/35 [==============================] - 328s 9s/step - loss: 0.4730 - accuracy: 0.7739\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "train_x, train_y_hateful, train_y_aggressive, train_y_targeted, test_x, test_y_hateful, test_y_aggressive, test_y_targeted, max_sent = get_data(\n",
    "        train_file='/Users/lakshmivarshitha/Downloads/train_es.tsv',\n",
    "        test_file='/Users/lakshmivarshitha/Downloads/dev_es.tsv',\n",
    "        padding='post')\n",
    "\n",
    "train_x = np.asarray(train_x)\n",
    "train_y = np.asarray(train_y_hateful)\n",
    "train_y = np.reshape(train_y, (-1, 1))\n",
    "test_x = np.asarray(test_x)\n",
    "test_y = np.asarray(test_y_hateful)\n",
    "test_y = np.reshape(test_y, (-1, 1))\n",
    "\n",
    "recurrent_model = Sequential()\n",
    "recurrent_model.add(Embedding(input_dim=5000, output_dim=28, input_length=max_sent, mask_zero=False))\n",
    "recurrent_model.add(LSTM(units=64, return_sequences=True, recurrent_dropout=0.1))\n",
    "recurrent_model.add(Flatten())\n",
    "\n",
    "recurrent_model.add(Dense(units=1, activation='sigmoid'))\n",
    "recurrent_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "plot_model(recurrent_model, to_file=\"/content/embedded_lstm.png\", show_shapes=True, show_dtype=False, show_layer_names=False, show_trainable=True, show_layer_activations=True)\n",
    "\n",
    "# Set callback functions to early stop training and save the best model so far\n",
    "es = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Train neural network\n",
    "    # recurrent_model.fit(train_x, train_y, validation_split=0.2, epochs=100, callbacks=[es], batch_size=64)\n",
    "time1 = time.ctime()\n",
    "recurrent_model.fit(train_x, train_y, callbacks=[es], epochs=20, batch_size=128)\n",
    "time2 = time.ctime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd2e9f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 5s 311ms/step\n",
      "[0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 5996, 28)          140000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 5996, 64)          23808     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 383744)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 383745    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 547553 (2.09 MB)\n",
      "Trainable params: 547553 (2.09 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Precision: 0.6404712691194708 \n",
      " Recall: 0.6376466394452006 \n",
      " F1-score: 0.6382897850583028 n Accuracy: 0.646\n",
      "Start time:Wed Nov 29 22:14:31 2023 \n",
      " End time:Thu Nov 30 00:02:28 2023\n"
     ]
    }
   ],
   "source": [
    "recurrent_model.save_weights('/Users/lakshmivarshitha/Desktop/embedded_lstm_weights.h5')\n",
    "\n",
    "y_test_pred_hateful = recurrent_model.predict(test_x)\n",
    "\n",
    "\n",
    "smaller_than = 0.499\n",
    "list_of_numbers = list()\n",
    "\n",
    "for y in y_test_pred_hateful:\n",
    "        #print(int(y))\n",
    "    if y <= smaller_than:\n",
    "        list_of_numbers.append(int(\"0\"))\n",
    "    else:\n",
    "        list_of_numbers.append(int(\"1\"))\n",
    "\n",
    "print(list_of_numbers)\n",
    "\n",
    "test_y_new = []\n",
    "for x in test_y:\n",
    "    x_new = float(x)\n",
    "    x_new = np.asarray(x_new)\n",
    "    test_y_new.append(x_new)\n",
    "\n",
    "accuracy = accuracy_score(test_y_new, y_test_pred_hateful.round())\n",
    "prec = precision_score(test_y_new, y_test_pred_hateful.round(), average='macro')\n",
    "rec = recall_score(test_y_new, y_test_pred_hateful.round(), average='macro')\n",
    "f1 = f1_score(test_y_new, y_test_pred_hateful.round(), average='macro')\n",
    "#print(y_test_pred_hateful)\n",
    "\n",
    "\n",
    "print(recurrent_model.summary())\n",
    "print(\"Precision:\", prec, \"\\n Recall:\", rec, \"\\n F1-score:\", f1, \"n Accuracy:\", accuracy)\n",
    "print(f\"Start time:{time1} \\n End time:{time2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba6271a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your sentence: eres hermosa\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "The sentence is: Hateful\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Flatten, Dense\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "\n",
    "def load_model(max_sent, weights_path):\n",
    "    # Create the model architecture - this has to match the architecture of the trained model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=5000, output_dim=28, input_length=max_sent, mask_zero=False))\n",
    "    model.add(LSTM(units=64, return_sequences=True, recurrent_dropout=0.1))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    # Load the trained weights\n",
    "    model.load_weights(weights_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_max_sentence_and_alphabet(train_file):\n",
    "  list_of_sentences_train = list()\n",
    "  max_length = list()\n",
    "\n",
    "    ### Train file ###\n",
    "  with open(train_file, encoding=\"utf8\") as cur_train_file:\n",
    "      reader = csv.reader(cur_train_file, delimiter='\\t')\n",
    "      for row in reader:\n",
    "          list_of_sentences_train.append(row[1])\n",
    "\n",
    "  list_of_sentences_train.pop(0)\n",
    "\n",
    "  for x in list_of_sentences_train:\n",
    "      max_length.append(len(x))\n",
    "  longest_sent = max(max_length)\n",
    "\n",
    "  alphabet = dict()\n",
    "\n",
    "  for sent in list_of_sentences_train:\n",
    "      for char in sent:\n",
    "          if char not in alphabet:\n",
    "            alphabet[char] = len(alphabet) + 1\n",
    "  return alphabet, longest_sent\n",
    "\n",
    "def preprocess_input(sentence, alphabet, max_sent):\n",
    "    # Convert each character in the sentence to its corresponding integer\n",
    "    sentence_integers = []\n",
    "    alphabet_length = len(alphabet)\n",
    "    for char in sentence:\n",
    "        if char in alphabet:\n",
    "            sentence_integers.append(alphabet[char])\n",
    "        else:\n",
    "            unk = alphabet_length + 1  # For unknown characters\n",
    "            sentence_integers.append(unk)\n",
    "\n",
    "    # Pad the sequence to the maximum length used during training\n",
    "    padded_seq = pad_sequences([sentence_integers], maxlen=max_sent, padding='post', value=0)\n",
    "\n",
    "    return padded_seq\n",
    "\n",
    "\n",
    "# Usage example\n",
    "weights_path = '/Users/lakshmivarshitha/Desktop/embedded_lstm_weights.h5'  # Path to your saved weights\n",
    "\n",
    "alphabet, max_sent = get_max_sentence_and_alphabet(\"/Users/lakshmivarshitha/Downloads/train_es.tsv\")\n",
    "#print(f\"maximum length of a sentence = {max_sent}/nAlphabet dictionary : {alphabet}\")\n",
    "model = load_model(max_sent, weights_path)\n",
    "\n",
    "# Example usage\n",
    "sentence = input(\"Enter your sentence: \")\n",
    "\n",
    "processed_sentence = preprocess_input(sentence, alphabet, max_sent)\n",
    "prediction = model.predict(processed_sentence)\n",
    "predicted_label = \"Hateful\" if prediction[0][0] > 0.5 else \"Non-Hateful\"\n",
    "print(f\"The sentence is: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49191f80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
