{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X9EBLtD22Jgw"
      },
      "outputs": [],
      "source": [
        "from keras import Sequential\n",
        "\n",
        "model = Sequential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Un9TI3JI9JE9"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "from keras import regularizers\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.layers import Dense, Dropout, Embedding, GRU, SimpleRNN, RNN, LSTM, Flatten, Activation\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, precision_recall_fscore_support\n",
        "# from baseline import bong\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(train_file, test_file, padding='post'):\n",
        "    list_of_sentences_train = list()\n",
        "    list_of_hateful_train = list()\n",
        "    list_of_aggressive_train = list()\n",
        "    list_of_targeted_train = list()\n",
        "    max_length = list()\n",
        "\n",
        "    ### Train file ###\n",
        "    with open(train_file, encoding=\"utf8\") as cur_train_file:\n",
        "        reader = csv.reader(cur_train_file, delimiter='\\t')\n",
        "        for row in reader:\n",
        "            list_of_sentences_train.append(row[1])\n",
        "            list_of_hateful_train.append(row[2])\n",
        "            list_of_targeted_train.append(row[3])\n",
        "            list_of_aggressive_train.append(row[4])\n",
        "\n",
        "    list_of_sentences_train.pop(0)\n",
        "    list_of_hateful_train.pop(0)\n",
        "    list_of_targeted_train.pop(0)\n",
        "    list_of_aggressive_train.pop(0)\n",
        "\n",
        "    for x in list_of_sentences_train:\n",
        "        max_length.append(len(x))\n",
        "    longest_sent = max(max_length)\n",
        "\n",
        "    alphabet = dict()\n",
        "    train_words = []\n",
        "\n",
        "    for sent in list_of_sentences_train:\n",
        "        words = []\n",
        "        for char in sent:\n",
        "            if char in alphabet:\n",
        "                words.append(alphabet[char])\n",
        "            else:\n",
        "                alphabet[char] = len(alphabet) + 1\n",
        "                words.append(alphabet[char])\n",
        "\n",
        "        train_words.append(words)\n",
        "\n",
        "    train_x = train_words\n",
        "    #train_y_hateful = list_of_hateful_train\n",
        "    train_y_hateful = [int(label) for label in list_of_hateful_train]\n",
        "    train_y_targeted = list_of_targeted_train\n",
        "    train_y_aggressive = list_of_aggressive_train\n",
        "\n",
        "    train_x = pad_sequences(train_x, padding=padding, value=0, maxlen=longest_sent, truncating='post')\n",
        "\n",
        "    ### TEST FIlE ###\n",
        "\n",
        "    list_of_sentences_test = list()\n",
        "    list_of_hateful_test = list()\n",
        "    list_of_aggressive_test = list()\n",
        "    list_of_targeted_test = list()\n",
        "\n",
        "    with open(test_file, encoding=\"utf8\") as cur_test_file:\n",
        "        reader = csv.reader(cur_test_file, delimiter='\\t')\n",
        "        for row in reader:\n",
        "            list_of_sentences_test.append(row[1])\n",
        "            list_of_hateful_test.append(row[2])\n",
        "            list_of_aggressive_test.append(row[3])\n",
        "            list_of_targeted_test.append(row[4])\n",
        "\n",
        "        list_of_sentences_test.pop(0)\n",
        "        list_of_hateful_test.pop(0)\n",
        "        list_of_aggressive_test.pop(0)\n",
        "        list_of_targeted_test.pop(0)\n",
        "\n",
        "        test_words = []\n",
        "        alphabet_length = len(alphabet)\n",
        "\n",
        "        for test_sent in list_of_sentences_test:\n",
        "            words = []\n",
        "            for char_test in test_sent:\n",
        "                if char_test in alphabet:\n",
        "                    words.append(alphabet[char_test])\n",
        "                else:\n",
        "                    unk = alphabet_length + 1\n",
        "                    words.append(unk)\n",
        "\n",
        "            test_words.append(words)\n",
        "\n",
        "        test_x = test_words\n",
        "        #test_y_hateful = list_of_hateful_test\n",
        "        test_y_hateful = [int(label) for label in list_of_hateful_test]\n",
        "        test_y_aggressive = list_of_aggressive_test\n",
        "        test_y_targeted = list_of_targeted_test\n",
        "\n",
        "        test_x = pad_sequences(test_x, padding=padding, value=0, maxlen=longest_sent, truncating='post')\n",
        "\n",
        "        print(len(train_x), len(train_y_hateful),len(train_y_aggressive), len(train_y_targeted), len(test_x),\n",
        "             len(test_y_hateful), len(test_y_aggressive), len(test_y_targeted))\n",
        "\n",
        "        return train_x, train_y_hateful, train_y_aggressive, train_y_targeted, test_x, test_y_hateful, test_y_aggressive, test_y_targeted, longest_sent"
      ],
      "metadata": {
        "id": "3YFtP6Ru96pZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time1 = time.ctime()\n",
        "\n",
        "seed = 7\n",
        "np.random.seed(seed)"
      ],
      "metadata": {
        "id": "D2bqtZUEBD63"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recurrent_network_hateful():\n",
        "    train_x, train_y_hateful, train_y_aggressive, train_y_targeted, test_x, test_y_hateful, test_y_aggressive, test_y_targeted, max_sent = get_data(\n",
        "        train_file='/content/train_en.tsv',\n",
        "        test_file='/content/dev_en.tsv',\n",
        "        padding='post')\n",
        "\n",
        "    train_x = np.asarray(train_x)\n",
        "    # train_x = train_x[:, :, np.newaxis]\n",
        "    train_y = np.asarray(train_y_hateful)\n",
        "    train_y = np.reshape(train_y, (-1, 1))\n",
        "    test_x = np.asarray(test_x)\n",
        "    # test_x = test_x[:, :, np.newaxis]\n",
        "    test_y = np.asarray(test_y_hateful)\n",
        "    test_y = np.reshape(test_y, (-1, 1))\n",
        "\n",
        "    recurrent_model = Sequential()\n",
        "    recurrent_model.add(Embedding(input_dim=5000, output_dim=28, input_length=max_sent, mask_zero=False))\n",
        "    recurrent_model.add(LSTM(units=64, return_sequences=True, recurrent_dropout=0.1))\n",
        "    #recurrent_model.add(LSTM(units=64, return_sequences=True, recurrent_dropout=0.2))\n",
        "    # recurrent_model.add(Dropout(0.2))\n",
        "    recurrent_model.add(Flatten())\n",
        "\n",
        "    recurrent_model.add(Dense(units=1, activation='sigmoid'))\n",
        "    recurrent_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Set callback functions to early stop training and save the best model so far\n",
        "    es = EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "    # Train neural network\n",
        "    # recurrent_model.fit(train_x, train_y, validation_split=0.2, epochs=100, callbacks=[es], batch_size=64)\n",
        "    recurrent_model.fit(train_x, train_y, callbacks=[es], epochs=50, batch_size=128)\n",
        "\n",
        "    recurrent_model.save_weights('/content/embedded_lstm_weights.h5')\n",
        "\n",
        "    y_test_pred_hateful = recurrent_model.predict(test_x)\n",
        "\n",
        "\n",
        "    smaller_than = 0.499\n",
        "    list_of_numbers = list()\n",
        "\n",
        "    for y in y_test_pred_hateful:\n",
        "        #print(int(y))\n",
        "        if y <= smaller_than:\n",
        "            list_of_numbers.append(int(\"0\"))\n",
        "        else:\n",
        "            list_of_numbers.append(int(\"1\"))\n",
        "\n",
        "    #print(list_of_numbers)\n",
        "\n",
        "    test_y_new = []\n",
        "    for x in test_y:\n",
        "        x_new = float(x)\n",
        "        x_new = np.asarray(x_new)\n",
        "        test_y_new.append(x_new)\n",
        "\n",
        "    prec = precision_score(test_y_new, y_test_pred_hateful.round(), average='macro')\n",
        "    rec = recall_score(test_y_new, y_test_pred_hateful.round(), average='macro')\n",
        "    f1 = f1_score(test_y_new, y_test_pred_hateful.round(), average='macro')\n",
        "    print(y_test_pred_hateful)\n",
        "    time2 = time.ctime()\n",
        "\n",
        "    print(recurrent_model.summary())\n",
        "    print(\"Precision:\", prec, \"\\n Recall:\", rec, \"\\n F1-score:\", f1)\n",
        "    print(time1 + '\\n' + time2)\n",
        "\n",
        "    return y_test_pred_hateful"
      ],
      "metadata": {
        "id": "YKLy3e2OAggZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recurrent_network_hateful()"
      ],
      "metadata": {
        "id": "CwhmqxfnB2tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Flatten, Dense\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import keras\n",
        "\n",
        "def load_model(max_sent, weights_path):\n",
        "    # Create the model architecture - this has to match the architecture of the trained model\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=5000, output_dim=28, input_length=max_sent, mask_zero=False))\n",
        "    model.add(LSTM(units=64, return_sequences=True, recurrent_dropout=0.1))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "    # Load the trained weights\n",
        "    model.load_weights(weights_path)\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_max_sentence_and_alphabet(train_file):\n",
        "  list_of_sentences_train = list()\n",
        "  max_length = list()\n",
        "\n",
        "    ### Train file ###\n",
        "  with open(train_file, encoding=\"utf8\") as cur_train_file:\n",
        "      reader = csv.reader(cur_train_file, delimiter='\\t')\n",
        "      for row in reader:\n",
        "          list_of_sentences_train.append(row[1])\n",
        "\n",
        "  list_of_sentences_train.pop(0)\n",
        "\n",
        "  for x in list_of_sentences_train:\n",
        "      max_length.append(len(x))\n",
        "  longest_sent = max(max_length)\n",
        "\n",
        "  alphabet = dict()\n",
        "\n",
        "  for sent in list_of_sentences_train:\n",
        "      for char in sent:\n",
        "          if char not in alphabet:\n",
        "            alphabet[char] = len(alphabet) + 1\n",
        "  return alphabet, longest_sent\n",
        "\n",
        "def preprocess_input(sentence, alphabet, max_sent):\n",
        "    # Convert each character in the sentence to its corresponding integer\n",
        "    sentence_integers = []\n",
        "    alphabet_length = len(alphabet)\n",
        "    for char in sentence:\n",
        "        if char in alphabet:\n",
        "            sentence_integers.append(alphabet[char])\n",
        "        else:\n",
        "            unk = alphabet_length + 1  # For unknown characters\n",
        "            sentence_integers.append(unk)\n",
        "\n",
        "    # Pad the sequence to the maximum length used during training\n",
        "    padded_seq = pad_sequences([sentence_integers], maxlen=max_sent, padding='post', value=0)\n",
        "\n",
        "    return padded_seq\n",
        "\n",
        "\n",
        "# Usage example\n",
        "weights_path = '/content/embedded_lstm_weights.h5'  # Path to your saved weights\n",
        "\n",
        "alphabet, max_sent = get_max_sentence_and_alphabet(\"/content/train_en.tsv\")\n",
        "#print(f\"maximum length of a sentence = {max_sent}/nAlphabet dictionary : {alphabet}\")\n",
        "model = load_model(max_sent, weights_path)\n",
        "\n",
        "# Example usage\n",
        "sentence = input(\"Enter your sentence: \")\n",
        "\n",
        "processed_sentence = preprocess_input(sentence, alphabet, max_sent)\n",
        "prediction = model.predict(processed_sentence)\n",
        "predicted_label = \"Hateful\" if prediction[0][0] > 0.5 else \"Non-Hateful\"\n",
        "print(f\"The sentence is: {predicted_label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0nFW8r_En1f",
        "outputId": "5659fab4-6f62-4fb8-c7c4-f719fd64e3ca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your sentence: you are a madman\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "The sentence is: Hateful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GtgPSBZENfPU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}